{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Honours fasta processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "date_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "from onekp_func import onekp\n",
    "from phyto_func import phyto\n",
    "from append_file import append_file\n",
    "from log_func import log\n",
    "from species_fix import sp_fix\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually run unique_id.py, this is done so the file is not overwrittenbfrom random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate datasets, merge and standardise from Phytozome and OneKp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs the phyto and onekp functions doing the initial processing \n",
    "# These functions take in a file list of different blast searches to be combined. \n",
    "# The file must be in the format of fasta \\t details \\t protein \\n\n",
    "\n",
    "phyto('phyto_filelist')\n",
    "onekp('onekp_filelist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just joins the two output fasta files from both functions by appending to new 'master' files\n",
    "\n",
    "master = {'master':['phytozome_new_all', 'onekp_new_all'], \n",
    "'master_blinded':['phytozome_blinded_all','onekp_blinded_all']}\n",
    "\n",
    "for k,v in master.items():\n",
    "    append_file(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merges remaining phyotzome and onekp files, creates new columns so that the files can be concatenated\n",
    "\n",
    "ph_index = pd.read_csv('phytozome_master.txt', sep='\\t')\n",
    "o_index = pd.read_csv('onekp_master.txt', sep='\\t')\n",
    "ph_details = pd.read_csv('phytozome_details_mod_all_sp_corrected.txt', sep='\\t')\n",
    "ph_details['Unid'] = ph_details['Unid'].apply(str)\n",
    "sp = pd.read_csv('onekp_species_list.csv')\n",
    "\n",
    "# Adds the species to the onekp details dataframe\n",
    "o_index = pd.merge(o_index, sp, how = 'inner', on='Onekp_index_id')\n",
    "log(f'Species and taxonomic info from onekp_species_list.csv was added to Onekp_index_id dataframe')\n",
    "\n",
    "# Adds the species to the phytozome details dataframe\n",
    "ph_species = ph_details[['Unid', 'Species']]\n",
    "ph_species['Unid'] = ph_species['Unid'].apply(pd.to_numeric) \n",
    "ph_index = pd.merge(ph_index, ph_species, how='inner', on='Unid')\n",
    "log(f'Species names from phytozome_details_mod_all_sp_corrected.txt was added to phytozome_master dataframe')\n",
    "\n",
    "# Creates new columns in each dataframe so that they match \n",
    "ph_index['Onekp_index_id'] = np.nan\n",
    "ph_index['Scaffold'] = np.nan\n",
    "ph_index['Tax_1'] = np.nan\n",
    "ph_index['Tax_2'] = np.nan\n",
    "ph_index['PACid'] = ph_index['PACid'].apply(str)\n",
    "\n",
    "o_index['PACid'] = np.nan\n",
    "\n",
    "master_index = pd.concat([ph_index, o_index])\n",
    "\n",
    "\n",
    "master_index.to_csv('master_index.txt', sep='\\t', index=False)\n",
    "\n",
    "log(f'Onekp_index_id and phytozome_master dataframes were concatednated with a total of {len(master_index)} sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a working ('w') dataframe with short seqs removed\n",
    "\n",
    "# Variables defining 70% length of AtSAL variants, without signal peptide, and the length coefficient \n",
    "coef = 0.7\n",
    "SAL1 = 353\n",
    "SAL2 = 347\n",
    "SAL3 = 357\n",
    "SAL4 = 345\n",
    "AHL = 373\n",
    "Unid = 397\n",
    "\n",
    "log('Sequences filtered by length (A. thaliana version minus signal peptide) as follows: \\n' +\n",
    "    f'                        SAl1 = {SAL1}\\n' +\n",
    "    f'                        SAl2 = {SAL2}\\n' +\n",
    "    f'                        SAL3 = {SAL3}\\n' +\n",
    "    f'                        SAL4 = {SAL4}\\n' +\n",
    "    f'                        AHL  = {AHL}\\n' +\n",
    "    f'                        Unid = {Unid}\\n' +\n",
    "    f'                        multiplied by a coefficient of {coef}')\n",
    "\n",
    "orig = pd.read_csv('master_index.txt', sep='\\t')\n",
    "\n",
    "# Creates the dataframe of filtered sequences\n",
    "w_index =  pd.concat([\n",
    "    orig[(orig['SAL_variant'] == 'SAL1') & (orig['Sequence'].str.len() >= (SAL1 * coef))],\n",
    "    orig[(orig['SAL_variant'] == 'SAL2') & (orig['Sequence'].str.len() >= (SAL2 * coef))],\n",
    "    orig[(orig['SAL_variant'] == 'SAL3') & (orig['Sequence'].str.len() >= (SAL3 * coef))],\n",
    "    orig[(orig['SAL_variant'] == 'SAL4') & (orig['Sequence'].str.len() >= (SAL4 * coef))],\n",
    "    orig[(orig['SAL_variant'] == 'AHL') & (orig['Sequence'].str.len() >= (AHL * coef))],\n",
    "    orig[(orig['SAL_variant'] == 'Unid') & (orig['Sequence'].str.len() >= (Unid * coef))],\n",
    "                     ])\n",
    "\n",
    "log(f'{len(w_index)} sequences were >= to the above parameters (working set)')\n",
    "\n",
    "# Creates a dataframe of the dropped sequences (not kept), just to generate a count, for validation purposes\n",
    "d_index =  pd.concat([\n",
    "    orig[(orig['SAL_variant'] == 'SAL1') & (orig['Sequence'].str.len() < (SAL1 * coef))],\n",
    "    orig[(orig['SAL_variant'] == 'SAL2') & (orig['Sequence'].str.len() < (SAL2 * coef))],\n",
    "    orig[(orig['SAL_variant'] == 'SAL3') & (orig['Sequence'].str.len() < (SAL3 * coef))],\n",
    "    orig[(orig['SAL_variant'] == 'SAL4') & (orig['Sequence'].str.len() < (SAL4 * coef))],\n",
    "    orig[(orig['SAL_variant'] == 'AHL') & (orig['Sequence'].str.len() < (AHL * coef))],\n",
    "    orig[(orig['SAL_variant'] == 'Unid') & (orig['Sequence'].str.len() < (Unid * coef))],\n",
    "                     ])\n",
    "\n",
    "log(f'{len(d_index)}  sequences were < the above parameters (discard set)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the w_index dataframe from above and creates a list of the unique id's from it\n",
    "w_unid = []\n",
    "for i in w_index['Unid']:\n",
    "    w_unid.append(str(i))\n",
    "\n",
    "log(f'{len(w_unid)} unique ids were listed from the working set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the master fasta file with only unique ids as headers and creates a dictionary with key = unique id and \n",
    "# values = sequence\n",
    "\n",
    "with open ('master_blinded.txt', 'r') as blinded:\n",
    "\n",
    "    temp_dict = {}\n",
    "    k = ''\n",
    "    v = ''\n",
    "    countw = 0\n",
    "    countd = 0\n",
    "\n",
    "    w_bofile = open('w_blinded.txt', 'w')\n",
    "    d_bofile = open('d_blinded.txt', 'w')\n",
    "\n",
    "    for line in blinded:\n",
    "        if line.startswith('>'):\n",
    "            k = line.strip('\\n').lstrip('>')\n",
    "        else:\n",
    "            v = line.strip('\\n')\n",
    "        temp_dict.update({k: v}) \n",
    "\n",
    "    # Creates two dictionaries based off whether the unique id was found in the list generated in the cell above\n",
    "    w_bdict = {k: v for k, v in temp_dict.items() if k in w_unid}\n",
    "    d_bdict = {k: v for k, v in temp_dict.items() if k not in w_unid}\n",
    "\n",
    "    # Writes two new fasta files, one with the unique ids found in the list, one without\n",
    "    for k,v in w_bdict.items():\n",
    "        w_bofile.write('>' + k + '\\n')\n",
    "        w_bofile.write(v + '\\n')\n",
    "        countw += 1\n",
    "    for k,v in d_bdict.items():\n",
    "        d_bofile.write('>' + k + '\\n')\n",
    "        d_bofile.write(v + '\\n')\n",
    "        countd += 1\n",
    "        \n",
    "    w_bofile.close()\n",
    "    d_bofile.close()  \n",
    "\n",
    "    log(f'w_blinded.txt was created with {countw} sequences from the unique id listed from the working set, derived from master_blinded.txt')\n",
    "    log(f'd_blinded.txt was created with {countd} sequences absent from the unique id listed from the working set, derived from master_blinded.txt')\n",
    "  \n",
    "    # pretty much the same as above but for fasta sequences with the orignal (+ unid) headers\n",
    "with open ('master.txt', 'r') as master:\n",
    "    \n",
    "    temp_dict = {}\n",
    "    k = ''\n",
    "    v = ''\n",
    "    countw = 0\n",
    "    countd = 0\n",
    "    \n",
    "    w_ofile = open('w.txt', 'w')\n",
    "    d_ofile = open('d.txt', 'w')\n",
    "    \n",
    "    for line in master:\n",
    "        if line.startswith('>'):\n",
    "            k = line.strip('\\n')\n",
    "        else:\n",
    "            v = line.strip('\\n')\n",
    "        temp_dict.update({k: v})\n",
    "        \n",
    "    w_dict = {k: v for k, v in temp_dict.items() if k.split()[0].lstrip('>') in w_unid}\n",
    "    d_dict = {k: v for k, v in temp_dict.items() if k.split()[0].lstrip('>') not in w_unid}\n",
    "\n",
    "    w_ofile = open('w.txt', 'w')\n",
    "    d_ofile = open('d.txt', 'w')\n",
    "    \n",
    "    for k,v in w_dict.items():\n",
    "        w_ofile.write(k + '\\n')\n",
    "        w_ofile.write(v + '\\n')\n",
    "        countw += 1\n",
    "\n",
    "    for k,v in d_dict.items():\n",
    "        d_ofile.write(k + '\\n')\n",
    "        d_ofile.write(v + '\\n')\n",
    "        countd += 1\n",
    "        \n",
    "    w_ofile.close()\n",
    "    d_ofile.close()\n",
    "\n",
    "    log(f'w.txt was created with {countw} sequences from the unique id listed from the working set, derived from master.txt')\n",
    "    log(f'd.txt was created with {countd} sequences absent from the unique id listed from the working set, derived from master.txt')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Creates six files, sorting the sequences from the w_blinded file created above, into the six different proteins\n",
    "\n",
    "import pandas as pd\n",
    "from log_func import log\n",
    "\n",
    "orig = pd.read_csv('master_index.txt', sep='\\t')\n",
    "\n",
    "# Creates a dataframe (var) from the master index, with only the three coloumns listed below\n",
    "var = orig[['Unid', 'SAL_variant', 'Datasource']]\n",
    "var['Unid'] = var['Unid'].apply(str)\n",
    "\n",
    "with open ('w_blinded.txt', 'r') as blinded:\n",
    "\n",
    "    temp_dict = {}\n",
    "    k = ''\n",
    "    v = ''\n",
    "    countw = 0\n",
    "    \n",
    "    # Creates a dictionary of all sequences in the file key = unique id and values = sequences\n",
    "    for line in blinded:\n",
    "        if line.startswith('>'):\n",
    "            k = line.strip('\\n').lstrip('>')\n",
    "        else:\n",
    "            v = line.strip('\\n')\n",
    "        temp_dict.update({k: v}) \n",
    "    \n",
    "    # Converts the dictionary above into a dataframe\n",
    "    tempDF = pd.DataFrame.from_dict(temp_dict, orient='index')\n",
    "    tempDF.reset_index(level=0, inplace=True)\n",
    "    tempDF.columns = ['Unid', 'Sequence']\n",
    "    # Merges the var dataframe with the one above from the filtered sequences\n",
    "    fasta = var.merge(tempDF, on=['Unid'], how='inner').fillna('NaN')\n",
    "    \n",
    "    # Creates six new dataframes from the new one created above (fasta), each filtered by one of the proteins \n",
    "    ahl = fasta[fasta['SAL_variant'] == 'AHL']\n",
    "    sal1 = fasta[fasta['SAL_variant'] == 'SAL1']\n",
    "    sal2 = fasta[fasta['SAL_variant'] == 'SAL2']\n",
    "    sal3 = fasta[fasta['SAL_variant'] == 'SAL3']\n",
    "    sal4 = fasta[fasta['SAL_variant'] == 'SAL4']\n",
    "    unid = fasta[fasta['SAL_variant'] == 'Unid']\n",
    "    \n",
    "import os # Allows later deletion of the temporary file that is created\n",
    "\n",
    "def var():\n",
    "    '''\n",
    "    Writes each of the protein specific dataframes above to new files \n",
    "    \n",
    "    Takes no arguments\n",
    "    '''\n",
    "\n",
    "    protein = {'ahl': ahl, 'sal1': sal1, 'sal2': sal2, 'sal3': sal3, 'sal4': sal4, 'unid': unid}\n",
    "    \n",
    "    for k,v in protein.items():\n",
    "        name = k\n",
    "        var = v\n",
    "\n",
    "\n",
    "        ofile = open(f'w_blinded_{name}.txt', 'w')\n",
    "\n",
    "        tempfile = open('w_blinded_temp.txt', 'w')\n",
    "\n",
    "        tempfile.write(var.to_string(index=False))\n",
    "\n",
    "        tempfile.close()\n",
    "\n",
    "        with open('w_blinded_temp.txt', 'r') as temp:\n",
    "            count = 0\n",
    "            ls = []\n",
    "            for line in temp:\n",
    "                if count > 0:\n",
    "                    ls = line.split()\n",
    "                    ofile.write('>' + ls[0] + '\\t' + ls[1]+ '\\t' + ls[2] + '\\n' + ls[3] + '\\n')\n",
    "                count += 1\n",
    "\n",
    "        ofile.close()\n",
    "        \n",
    "        log(f'w_blinded_{name}.txt was created with {len(var)} sequences')\n",
    "\n",
    "    os.remove('w_blinded_temp.txt')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "var()  # Run on 200828 but not used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate cluster list from SSN results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following unique IDs were mannually assigned to the sequences detailed below, and the master_index.txt file was manually updated and saved as master_index_w_outg_controls.txt\n",
    "\n",
    "17155: AtUnid, 27030: AtAHL, 36896: AtSAL1, 38488: AtSAL2, 44133: AtSAL2, 36454: AtSAL4, 52125: mammalian, 42852: mammalian, 87894: mammalian, 27803: mammalian, 23604: mammalian, 49932: mammalian, 53531: mammalian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 200826 Restart from w_blinded  \n",
    "I'm going to skip CDHit and trim using t_coffee, then resubmit to EFI for Cytoscape visulisation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ran:  \n",
    "t_coffee -other_pg seq_reformat -in w_blinded.fasta -action +trim _seq_%%90_O10 >200826_blinded_trimed.fasta   \n",
    "This is approx. 20k sequences and turns out is too large to run on flashheart  \n",
    "Submitted to CDHit at 0.95 with default values which reduced it to ~2800 sequences  \n",
    "This was then run on t_coffee with:  \n",
    "* t_coffee -other_pg seq_reformat -in 200826_cdhit_95.fasta -action +trim _seq_n1500 -output fasta_seq >200826_chit_95_trimmed.fasta  \n",
    "</br>  \n",
    "AtSAL and ordered sequences were added  \n",
    "* fasta file was submitted to EFI-EST 'Fasta (option C)', E-value = 1, fragments were left in  \n",
    "EFI-EST Job ID: 48003  \n",
    "Computation Type: FASTA (Option C), no FASTA header reading  \n",
    "Job Name: 200826_chit_95_trimed.fasta  \n",
    "Uploaded Fasta File: 200826_chit_95_trimed.fasta  \n",
    "E-Value: 1  \n",
    "Fraction: 1  \n",
    "</br>  \n",
    "  \n",
    "EST settings were non-selective (download all), I'll manuallly remove sequences of likely wrong lengths  \n",
    "Analysis Job ID: 58552  \n",
    "Minimum Length: 0  \n",
    "Maximum Length: 50000  \n",
    "Filter Type: E-Value  \n",
    "Filter Value: 3  \n",
    "Network Name: 200826_chit_95_trimed_fasta  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ordered and At sequences were remoeved first \n",
    "\n",
    "ahl = pd.read_csv('Output/200827_ssn/hax/200827_ahl.csv', sep=',')\n",
    "sal = pd.read_csv('Output/200827_ssn/hax/200827_SAL.csv', sep=',')\n",
    "other = pd.read_csv('Output/200827_ssn/hax/200827_other.csv', sep=',')\n",
    "unid = pd.read_csv('Output/200827_ssn/hax/200827_unid.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dictionary provides the two arguments for the 'clusters' function\n",
    "d = {'ahl': ahl, 'sal': sal, 'other': other, 'unid': unid}\n",
    "\n",
    "# Unpacks the dictionary interatively and passes the keys and values as the arguments for the 'clusters' function\n",
    "for k, v in d.items():\n",
    "    clusters(k, v, 'Output/master_blinded.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w_ahl_cluster.txt, w_sal_cluster.txt, w_other_cluster.txt were renamed to 200827_ahl.fasta, 200827_ahl.fasta, 200827_ahl.fasta, with the At and ordered sequences added back in and put in Alignments/200827"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 200830 Restart from w_blinded.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "200830 - took w_blinded.fasta and ran through cdhit at 90, then ran  \n",
    "t_coffee -reg -seq 200830_cdhit_90.fasta -nseq 100 -tree mbed -method mafftginsi_msa -outfile 200830_aln -outtree 200830_aln.mbed -thread=9  \n",
    "(-thread didn't do anything)\n",
    "Went through and removed suspected signal peptide, cut messy N-terminal sequences off and removed any short sequences - from the N-terminal short ones that did not start with M were deleted.  \n",
    "</br>\n",
    "EFI-EST Job ID: 48130  \n",
    "Computation Type: FASTA (Option C), no FASTA header reading  \n",
    "Job Name: 200830_aln_for_ssn.fasta  \n",
    "Uploaded Fasta File: 200830_aln_for_ssn.fasta  \n",
    "E-Value: 5  \n",
    "Fraction: 1  \n",
    "</br>\n",
    "Analysis Job ID: 58681  \n",
    "Minimum Length: 0  \n",
    "Maximum Length: 50000  \n",
    "Filter Type: E-Value  \n",
    "Filter Value: 10  \n",
    "Network Name: 200830_aln_ssn_fasta  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uses BioPython sequence cleaner script, modified for my use. Removes sequences with non-amino acid residues \n",
    "\n",
    "import sys\n",
    "from Bio import SeqIO\n",
    "\n",
    "def sequence_cleaner(fasta_file, por_n=0):\n",
    "    # Create our hash table to add the sequences\n",
    "    sequences={}\n",
    "    dropped={}\n",
    "    dropped_list =[]\n",
    "\n",
    "    # Using the Biopython fasta parse we can read our fasta input\n",
    "    for seq_record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        # Take the current sequence\n",
    "        sequence = str(seq_record.seq).upper()\n",
    "\n",
    "        # If the sequence passed in the test \"is it clean?\" and it isn't in the\n",
    "        # hash table, the sequence and its id are going to be in the hash\n",
    "        if (float(sequence.count(\"X\")) / float(len(sequence))) * 100 <= por_n:\n",
    "            if sequence not in sequences:\n",
    "                sequences[sequence] = seq_record.id   \n",
    "        else: dropped[sequence] = seq_record.id\n",
    "            \n",
    "    for k,v in dropped.items():\n",
    "        dropped_list.append(v)\n",
    "            \n",
    "\n",
    "    # Write the clean sequences\n",
    "\n",
    "    # Create a file in the same directory where I ran this script\n",
    "    with open(\"clear_\" + fasta_file, \"w+\") as output_file:\n",
    "        # Just read the hash table and write on the file as a fasta format\n",
    "        for sequence in sequences:\n",
    "            output_file.write(\">\" + sequences[sequence] + \"\\n\" + sequence + \"\\n\")\n",
    "    print(\"These sequences were dropped\")\n",
    "    print(dropped_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ran 200831\n",
    "\n",
    "sequence_cleaner('200830_aln_for_ssn.fasta')\n",
    "\n",
    "#Sequence cleaner is now as duplicate sequences may not be caught until signal peptides are removed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on 200831, returned:  \n",
    "These sequences were dropped  \n",
    "['69375', '60666', '34646', '40609']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters(k, v, master):\n",
    "    \"\"\"\n",
    "    Takes three arguments, the file with a list of unique ids, the master file of sequences, and the detail to add \n",
    "    in the header (the cluster it's derived from), from which the new file name is derived. \n",
    "    This function takes the Cytoscape table of sequences (as a df) and converts them to new files as a list of unique ids. \n",
    "    The files are named \"{colour}_cluster.txt\" as the list of unique ids in the the cluster and \"w_{colour}_cluster.txt\" \n",
    "    which is a fasta file of all sequences in that cluster\n",
    "    It opens and automatically closes the file using 'with open [...] as x', and logs the results in Honours_log.txt\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "       \n",
    "    temp_list = v['Description'].tolist()\n",
    "    temp_set = []\n",
    "    for i in temp_list:\n",
    "        t = i.split(\"|\")\n",
    "        for n in t:\n",
    "            n = int(n)\n",
    "            temp_set.append(n)\n",
    "    \n",
    "    temp_set = set(temp_set)\n",
    "\n",
    "    with open(f'{k}_cluster.txt', 'w') as temp:\n",
    "        for i in temp_set:\n",
    "            count += 1\n",
    "            temp.write(str(i) + '\\n')\n",
    "    \n",
    "    with open (master, 'r') as blinded:\n",
    "\n",
    "        temp_dict = {}\n",
    "        x = ''\n",
    "        y = ''\n",
    "        countw = 0\n",
    "\n",
    "        w_bofile = open(f'w_{k}_cluster.txt', 'a')\n",
    "\n",
    "        # Converts the master fasta file into a dictionary where the key is fasta header and the value is the sequence\n",
    "        for line in blinded:\n",
    "            if line.startswith('>'):\n",
    "                line = line[:6]\n",
    "                x = line.strip('\\n').lstrip('>')\n",
    "            else:\n",
    "                y = line.strip('\\n')\n",
    "            temp_dict.update({int(x): y}) \n",
    "    \n",
    "        # Creates a new dictionary based off whether the unique id was found in the orignal SSN cluster csv\n",
    "        w_bdict = {x: y for x, y in temp_dict.items() if x in temp_set}\n",
    "\n",
    "        \n",
    "        # Writes a new fasta file with the unique ids found in the dictionary w_bdict\n",
    "        for x, y in w_bdict.items():\n",
    "            w_bofile.write('>' + str(x) + ' ' + k + '\\n')\n",
    "            w_bofile.write(y + '\\n')\n",
    "            countw += 1\n",
    "\n",
    "        w_bofile.close()\n",
    "\n",
    "        log(f'w_{k}_cluster.txt was created/appended with {countw} sequences from the 200818_{k}.csv file, derived from {master}')\n",
    "            \n",
    "    log(f'{k}_cluster.txt was created with {count} unique IDs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes each of the Cytoscape sequence tables and reads them in pandas assigning them to df as their colours\n",
    "\n",
    "red = pd.read_csv('Output/200830_ssn/200830_red.csv', sep=',')\n",
    "blue = pd.read_csv('Output/200830_ssn/200830_blue.csv', sep=',')\n",
    "cyan = pd.read_csv('Output/200830_ssn/200830_cyan.csv', sep=',')\n",
    "pink = pd.read_csv('Output/200830_ssn/200830_pink.csv', sep=',')\n",
    "tan = pd.read_csv('Output/200830_ssn/200830_tan.csv', sep=',')\n",
    "green = pd.read_csv('Output/200830_ssn/200830_green.csv', sep=',')\n",
    "forest = pd.read_csv('Output/200830_ssn/200830_forest.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ran 200831\n",
    "\n",
    "# This dictionary provides the two arguments for the 'clusters' function\n",
    "d = {'blue': blue, 'cyan': cyan, 'pink': pink, 'tan': tan, 'red': red, 'green': green, 'forest': forest}\n",
    "\n",
    "# Unpacks the dictionary interatively and passes the keys and values as the arguments for the 'clusters' function\n",
    "for k, v in d.items():\n",
    "    clusters(k, v, 'Output/master_blinded.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renamed w_{colour}_cluster.txt files to {colour}.fasta and aligned using:  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_cluster(date):\n",
    "    \"\"\"\n",
    "    Takes the date as a string to generate fasta files from the clusters files, resultant fasta files have headers\n",
    "    in the following format: >Unid Species Tax_1. Sequences are taken from the index file\n",
    "    \"\"\"\n",
    "    \n",
    "    colours = ['blue', 'cyan', 'pink', 'tan', 'red', 'green', 'forest']\n",
    "\n",
    "    # Reads the master index file\n",
    "    index = pd.read_csv(\"master_index.txt\", sep=\"\\t\")\n",
    "\n",
    "    for colour in colours:\n",
    "        ofile = open(f'Output/200830_ssn/{colour}_cluster.txt', 'r')\n",
    "        seq_list = []\n",
    "\n",
    "        #Generates a list of Unids from the above file and populates seq_list as int (not string)\n",
    "        for line in ofile:\n",
    "            seq_list.append(int(line))\n",
    "\n",
    "        # Makes a new dataframe only if the 'Unid' is also found in seq_list\n",
    "        index_colour = index[index['Unid'].isin(seq_list)]  \n",
    "\n",
    "        # Creates a new dataframe from index_colour using the columns listed below\n",
    "        builder = index_colour[[\"Unid\", \"Sequence\", \"Species\", \"Tax_1\"]]    \n",
    "\n",
    "        # Creates a dictionary setting the 'Sequence' column as the index then passing that as the Key, \n",
    "        # while a list is created from the other columns as the Value\n",
    "        temp_dict = builder.set_index('Sequence').T.to_dict('list')    \n",
    "\n",
    "        # Writes a new fasta file with the header as >Unid Species Tax_1 \n",
    "        alnfile = open(f'{date}_{colour}.fasta', 'w')\n",
    "        \n",
    "        countw = 0\n",
    "        for k,v in temp_dict.items():\n",
    "            alnfile.write('>' + str(v[0]) + ' ' + str(v[1]) + ' ' + str(v[2]) + '\\n')\n",
    "            countw += 1\n",
    "            alnfile.write(k + '\\n')\n",
    "\n",
    "        alnfile.close()\n",
    "\n",
    "        ofile.close()\n",
    "        \n",
    "        log(f'{date}_{colour}.txt was created/appended with {countw} sequences from the 200830_ssn {colour}_cluster.txt file, sequences derived from mast_index.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_cluster(\"200901\") # Ran 200901, will align as above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t_coffee drops anything in a fasta header that is after a space. *anger *\n",
    "Below fixes that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to try to align all the sequences in 200830_cdhit_90.fasta and do the clustering manually "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_aln(file_name):\n",
    "    \"\"\"\n",
    "    Takes an aligned fasta file (input as string)and adds Species and Tax_1 info from \n",
    "    master_index.txt in the following format: >Unid |Species|Tax_1. \n",
    "    \"\"\"\n",
    "\n",
    "    # Reads the master index file\n",
    "    index = pd.read_csv(\"master_index.txt\", sep=\"\\t\")\n",
    "\n",
    "    aln_dict = {}\n",
    "    k = ''\n",
    "    v = ''\n",
    "\n",
    "    # Makes a dictionary from an alignment file with the Unid as the Key and the sequence as the Value\n",
    "    alnfile = open(f\"{file_name}\", 'r')     \n",
    "    \n",
    "    for line in alnfile:\n",
    "        if line.startswith('>'):\n",
    "            k = line.rstrip('\\n').lstrip('>')\n",
    "            k = int(k)\n",
    "        else:\n",
    "            v = line.strip('\\n')\n",
    "        aln_dict.update({k: v}) \n",
    "\n",
    "    alnfile.close()\n",
    "\n",
    "    #Creates a datafram out of the above dictionary with two columns, \"Unid\" and \"Sequence\"\n",
    "    aln = pd.DataFrame.from_dict(aln_dict, orient='index', columns=['Sequence'])\n",
    "    aln.index.name = \"Unid\"\n",
    "    aln = aln.reset_index()\n",
    "\n",
    "    # Creates a new dataframe from index_colour using the columns listed below to ultimately create the new fasta header\n",
    "    sp = index[[\"Unid\", \"Species\", \"Tax_1\"]]    \n",
    "\n",
    "    #Merges both the datafram with the aligned sequnce with the one directly above, on the Unid\n",
    "    builder = pd.merge(aln,sp,how='inner',on='Unid')\n",
    "\n",
    "    # Creates a dictionary setting the 'Sequence' column as the index then passing that as the Key, \n",
    "    # while a list is created from the other columns as the Value\n",
    "    temp_dict = builder.set_index('Sequence').T.to_dict('list')    \n",
    "\n",
    "    # Writes a new fasta file with the header as >Unid|Species|Tax_1 \n",
    "    alnfile2 = open(f'{file_name}_named.fasta', 'w')\n",
    "\n",
    "    countw = 0\n",
    "    for k,v in temp_dict.items():\n",
    "        alnfile.write('>' + str(v[0]) + '|' + str(v[1]) + '|' + str(v[2]) + '\\n')\n",
    "        countw += 1\n",
    "        alnfile.write(k + '\\n')\n",
    "\n",
    "    alnfile2.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added in the control sequences to sequence cleaner output, clear_200830_cdhit_90.fasta (renamed to 200901_all.fasta), and aligned using t_coffee regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_aln(\"200901_all_aln_w.fasta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "manually curtating the above then will continue manually clustering, note in methods identification of isoforms, and truncated sequences likely from endonucleases "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 200909 Final branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the phlyogeny from 200830 to go forward.   \n",
    "Removing outliers (long branch/poor align) / duplicates manually. Generated the list of unique ids of the ordered genes to keep, below:  \n",
    "(need to keep the sequences of genes we already have in the phylogeny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_dup = [52894, 65326, 90543, 73058, 23687, 61093, 76228, 88670, 65848, 73944, 22979, 52629, 71462, 45128, 74987]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First manually remove outlier sequences from the profile alignment (48803, 68977, 71379, 94733, 59845, 43533, 93150, 72690) and duplicate, 38676\n",
    "\n",
    "def raw_clust(cluster, profile):\n",
    "    \"\"\"\n",
    "    This function takes a fasta file ('cluster') and an alignment fasta file ('profile'), and finds the subset/intersection of sequences\n",
    "    in both fasta files then writes a new fasta file from that list with unmodified sequences taken from 'master_index.txt'\n",
    "    \"\"\"\n",
    "    clust_lst = []  \n",
    "    aln_lst = []\n",
    "\n",
    "    with open(f'{cluster}.fasta', 'r') as clust:\n",
    "        for line in clust:\n",
    "            if line.startswith('>'):\n",
    "                a = line.rstrip('\\n').lstrip('>')\n",
    "                clust_lst.append(a)\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    with open(f'{profile}.fasta', 'r') as pro_aln:\n",
    "        for line in pro_aln:\n",
    "            if line.startswith('>'):\n",
    "                b = line.rstrip('\\n').lstrip('>')\n",
    "                aln_lst.append(b)\n",
    "            else:\n",
    "                pass    \n",
    "            \n",
    "    #subset\n",
    "    subset = list(set(clust_lst) & set(aln_lst))\n",
    "    \n",
    "    index = pd.read_csv(\"master_index.txt\", sep=\"\\t\")\n",
    "    \n",
    "    # Makes a new index dataframe of unids found in subset\n",
    "    subset_df = index[index['Unid'].isin(subset)]\n",
    "\n",
    "    # Converts subset_df into a dictionary where the key is the unid and the value is the sequence\n",
    "    fdict = dict(zip(subset_df.Unid, subset_df.Sequence))\n",
    "\n",
    "    ofile = open(f'raw_{cluster}.txt', 'w')            \n",
    "    # Writes a new fasta file with the unique ids found in the dictionary fdict\n",
    "    for x, y in fdict.items():\n",
    "        ofile.write('>' + str(x) + '\\n')\n",
    "        ofile.write(y + '\\n')\n",
    "\n",
    "    ofile.close()      \n",
    "\n",
    "    log(f'raw_{cluster}.txt was created with {len(fdict)} sequences from the {cluster}.fasta alingment - part of the 200830 branch, derived from master_index.txt')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_clust(\"green\", \"200905_u_t1_mod\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the above for red, blue, green  \n",
    "aln with regression  \n",
    "t_coffee -reg -seq{file}.fasta -nseq 100 -tree mbed -method mafftginsi_msa -outfile raw_{file}_aln.fasta -outtree {file}.mbed -thread 9 \n",
    "curate - manually removed any seq with large insertions or deletions   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will manually remove outlier sequences from the profile alignment (48803, 68977, 71379, 94733, 59845, 43533, 93150, 72690) and duplicate, 38676 from the aligned files (red.fasta ect)  \n",
    "  \n",
    "Then run t-coffee trim on them  \n",
    "SAL 200 sequences, other two 150 each  \n",
    "t_coffee -other_pg seq_reformat -in [file].fasta -in2 ordered_to_keep.fasta -action +trim _seq_n200 -output fasta_seq -outfile red_trim.fasta  \n",
    "    this takes the fasta file and trims it to n200 (keeps 200 sequences) while making sure that sequences in 'ordered_to_keep.fasta' are retained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes trimmed clusters (not aligned), generates a litst of unids and then takes the aligned files and makes a new fasta with the intersection\n",
    "# this new fasta file has the same actual sequences as the trimmed file but uses the aligned versions from the orignal file\n",
    "\n",
    "def final_clust(cluster):\n",
    "\n",
    "    clust_lst = []  \n",
    "\n",
    "    with open(f'{cluster}_trim.fasta', 'r') as clust:\n",
    "        for line in clust:\n",
    "            if line.startswith('>'):\n",
    "                a = line.rstrip('\\n').lstrip('>')\n",
    "                a = int(a)\n",
    "                clust_lst.append(a)\n",
    "            else:\n",
    "                pass\n",
    "           \n",
    "    with open (f'{cluster}.fasta', 'r') as aln:\n",
    "\n",
    "        temp_dict = {}\n",
    "        x = ''\n",
    "        y = ''\n",
    "\n",
    "        # Converts the orignal, aligned and not trimmed, fasta file into a dictionary where the key is fasta header and the value is the sequence\n",
    "        for line in aln:\n",
    "            if line.startswith('>'):\n",
    "                line = line[:6]\n",
    "                x = line.strip('\\n').lstrip('>')\n",
    "            else:\n",
    "                y = line.strip('\\n')\n",
    "            temp_dict.update({int(x): y}) \n",
    "\n",
    "        # Creates a new dictionary based off whether the unique id was found in the orignal SSN cluster csv\n",
    "        fdict = {x: y for x, y in temp_dict.items() if x in clust_lst}\n",
    "\n",
    "        ofile = open(f'{cluster}_f.fasta', 'w')        \n",
    "        # Writes a new fasta file with the unique ids found in the dictionary w_bdict\n",
    "        for x, y in fdict.items():\n",
    "            ofile.write('>' + str(x) + '\\n')\n",
    "            ofile.write(y + '\\n')\n",
    "\n",
    "        ofile.close()    \n",
    "\n",
    "    log(f'{cluster}_f.txt was created with {len(fdict)} sequences from the {cluster}_trim.fasta file - part of the 200830 branch, derived from {cluster}.fasta alignment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clust(\"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "did profile alignment  \n",
    "then model finder using:  \n",
    "iqtree2 -s 200909_profile_aln.fasta -m MF -mset WAG,LG,JTT,UL2,UL3,EX_EHO,LG4M,LG4X,CF4 -T AUTO  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GADI tree handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copies all tree files that finished (i.e. filters dir that have 12 files in the folder (a necessary but not sufficent condition for a finished run) \n",
    "# and checks in the log file that the results were written out)\n",
    "\n",
    "ran = set()\n",
    "tree_lst = []\n",
    "tree_count = 0\n",
    "\n",
    "# Opens every dir in '200916_bulk1' and checks the number of file in the dir exceeds 11 \n",
    "for r, d, f in os.walk(\"200916_bulk1\"):\n",
    "    for i in f:\n",
    "        if len(f) > 11:\n",
    "            ran.add(r)  # If condition met, adds that dir to the list 'ran'\n",
    "        else:\n",
    "            pass\n",
    "# Creates a new list of truncated dir names (from list 'ran')    \n",
    "for i in ran:\n",
    "    dir = str(i)\n",
    "    dir = dir[13:]\n",
    "    \n",
    "    # Opens each log file from the dir names in the list 'dir' and checks to see if the str 'Analysis results written to', which is necessary for the \n",
    "    # tree building run to have completed. If this str is there, the dir is added to a new list ('tree_lst') of completed tree building runs\n",
    "    with open (f'200916_bulk1/{dir}/200909_profile_aln.fasta.log', 'r') as tree_log:\n",
    "        if 'Analysis results written to' in tree_log.read():\n",
    "            tree_lst.append(dir)\n",
    "        else:\n",
    "            print(str(dir) + \" failed\")\n",
    "\n",
    "# Takes the tree file from each completed run and copies it to a single tree file called 'all_tree.nwk'\n",
    "with open(f'200916_bulk1/all_trees.nwk', 'a') as op_file:\n",
    "\n",
    "    for i in tree_lst:\n",
    "        tree_count += 1\n",
    "        with open(f'200916_bulk1/{i}/200909_profile_aln.fasta.treefile', 'r') as temp_file:\n",
    "            for line in temp_file:\n",
    "                op_file.write(line)\n",
    "            op_file.write('\\n')\n",
    "            \n",
    "        # writes a file 'tree_lst_AU.txt' of each tree (dir name) that was copied to the above file, in the same order\n",
    "        with open(f'200916_bulk1/tree_lst_AU.txt', 'a') as record:\n",
    "            record.write(str(tree_count) + '\\t' + str(i) + '\\n')\n",
    "\n",
    "print(tree_count)\n",
    "print(len(ran))\n",
    "print(len(tree_lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSN file generation  \n",
    "The cells below take the sequences used in the initial phylogeny to determine the root and creates a new version of unaligned sequences with a detailed fasta header "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opens the revelant fasta file and saves off the unids in variable 'seq_list'\n",
    "\n",
    "ofile = open('200830_aln_for_ssn.fasta', 'r')\n",
    "\n",
    "seq_list = []\n",
    "\n",
    "for line in ofile:\n",
    "    if line.startswith('>'):\n",
    "        seq_list.append(line[1:].rstrip('\\n'))\n",
    "    \n",
    "print(str(len(seq_list)) + \" sequences total\")\n",
    "\n",
    "ofile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes 'seq_list' from above and finds all relevant entries in the master index\n",
    "index = pd.read_csv('master_index.txt', sep='\\t')\n",
    "\n",
    "# Makes a dataframe, 'ssn', that is a subset of index comprising only of rows with 'Unid' that match seq_list\n",
    "ssn = index[index['Unid'].isin(seq_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes column 'Unid' from type int to str \n",
    "ssn['Unid'] = ssn.Unid.astype(str)\n",
    "\n",
    "# Merges colums (must be type str) into a new column called 'new_head' where values from each column are seperated by '|' \n",
    "ssn['new_head'] = ssn[['Unid','Datasource', 'Species']].agg('|'.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a dictionary with column 'new_head' as the key and 'Sequence' as the variable\n",
    "temp_dict = dict(zip(ssn.new_head,ssn.Sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a fasta file from the above dict\n",
    "\n",
    "alnfile = open('fig_1_ssn.fasta', 'a')\n",
    "\n",
    "for k,v in temp_dict.items():\n",
    "    alnfile.write('>' + str(k) + '\\n')\n",
    "    alnfile.write(v + '\\n')\n",
    "\n",
    "alnfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make working index with full taxonomic info for figures  \n",
    "Rather than rely one the full index with ~3k sequences that are needed a subset index is created of only the 500 final sequences in the tree (not including the AtSAL sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Took final alignments of the three main clades from ~/Bioinformatics\\06.restart_200830\\02.Profile_alignment\\02.Main_profile_alignment\\01.Main_clades\\02.Final_alignment_profiles\n",
    "# blue_f.fasta\n",
    "# green_f.fasta\n",
    "# red_f.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_lst = []  \n",
    "files = ['red', 'green', 'blue']\n",
    "\n",
    "# Makes a list of all Unids in each of the clade fasta files\n",
    "for i in files:\n",
    "    with open(f'{i}_f.fasta', 'r') as clust:\n",
    "        for line in clust:\n",
    "            if line.startswith('>'):\n",
    "                a = line.rstrip('\\n').lstrip('>')\n",
    "                a = int(a)\n",
    "                clust_lst.append(a)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "index = pd.read_csv(\"master_index.txt\", sep=\"\\t\")\n",
    "\n",
    "# Makes a new index dataframe with only rows with Unids that match that from the list generated above\n",
    "wrk_index = index[index['Unid'].isin(clust_lst)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrk_index.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrk_index.to_csv('temp_index.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cells take the 1kp file 'annotations' of the taxonomic classifications for all species in the 1kp dataset, and merges it with the wrk_index dataframe created above, effectively \n",
    "redoing the taxonomic classification from the original index file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno = pd.read_csv(\"annotations.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merges the annotations df with the wrk_index df on the Species columns. This has the added benefit of annotating the taxonomic classification of some of the Phytozome species \n",
    "new_index = pd.merge(wrk_index,anno, on=['Species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index.to_csv('new_index.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually corrected new_index, now reloading\n",
    "new_index2 = pd.read_csv(\"new_index.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_index = new_index.drop_duplicates(subset=['Unid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_index.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compares df wrk_index with the new one getting that got rid of duplicates and creates a datadrame of rows that were lost. Below cells then add them back in for manual correction\n",
    "missing = pd.concat([wrk_index,min_index]).drop_duplicates(subset=['Unid'], keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_sp = missing['Unid'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_index = index[index['Unid'].isin(missing_sp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrk_index2 = pd.concat([min_index, missing_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrk_index2.to_csv('wrk_index.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual correction of wrk_index.csv, then reload, then generate list of classifications for factor level ordering in R\n",
    "class_lst = pd.read_csv(\"wrk_index.csv\", sep=\",\")\n",
    "B_class_lst = class_lst['Brief_Classification'].tolist()\n",
    "B_class_lst = set(B_class_lst)\n",
    "B_class_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wrk_index.csv was then manually curated and saved as wrk_index2.csv which will be used going forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Make a tree file of all the trees in the dir 01.top20_AU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#did not end up using the final consensus tree of these 20 trees.\n",
    "\n",
    "#Makes a list of files in the directory\n",
    "ran = []\n",
    "\n",
    "for r, d, f in os.walk(\"01.top20_AU\"):\n",
    "    ran = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('List of files: ')\n",
    "print(ran)\n",
    "print('')\n",
    "print('Number of files: ')\n",
    "print(len(ran))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_file(name, lst):\n",
    "    '''\n",
    "    This function takes a list of files and appends them to a new txt file.\n",
    "    The function takes two arguments, the name (without the file extension) of the new file\n",
    "    and a text document list of files without the file extension\n",
    "    '''\n",
    "    from log_func import log\n",
    "\n",
    "    with open(f'{name}.nwk', 'a') as op_file:\n",
    "\n",
    "        for i in lst:\n",
    "            count = 0\n",
    "            with open(f'01.top20_AU/{i}', 'r') as temp_file:\n",
    "                for line in temp_file:\n",
    "                    op_file.write(line + '\\n')\n",
    "                    count += 1\n",
    "   #             log(f'{count} lines were added to {name}.txt')\n",
    "\n",
    "    op_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_file(\"top20_AU\", ran) #take all 20 trees and makes one tree file out of all of them for use in consensus tree building "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find species that are in all three homologues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.read_csv('wrk_index2.csv', sep=',')\n",
    "index[\"Details\"] = index[\"Species\"] + \"\\t\" + index[\"Full_Classification\"] #creates a new column that merges the Species and Full_Classification columns\n",
    "index = index.sort_values(by = 'Details')\n",
    "\n",
    "files = ['red', 'green', 'blue'] #refers to the final fasta files that made up the 3 clades in the 200909_profile_aln.fasta file that was used to build the\n",
    "                                 # GADI trees. Does not include the AtSALs\n",
    "\n",
    "red_sp = []\n",
    "green_sp = []\n",
    "blue_sp = []\n",
    "\n",
    "df_list = []\n",
    "\n",
    "#Makes a list of all unids in fasta file i\n",
    "for i in files:\n",
    "    with open(f'10.Genes_for_expression/{i}_f.fasta', 'r') as clust:\n",
    "        clust_lst = []  \n",
    "        for line in clust:\n",
    "            if line.startswith('>'):\n",
    "                a = line.rstrip('\\n').lstrip('>')\n",
    "                a = str(a)\n",
    "                clust_lst.append(a)\n",
    "            else:\n",
    "                pass   \n",
    "\n",
    "#Makess dfs for each of the files i which are subsets of wrk_index2, and lists of the 'Details' columns of each of these dfs\n",
    "        if i == 'red':\n",
    "            df_red = index[index['Unid'].isin(clust_lst)]\n",
    "            red_sp = df_red[\"Details\"].to_list()\n",
    "        elif i == 'green':\n",
    "            df_green =  index[index['Unid'].isin(clust_lst)]\n",
    "            green_sp = df_green[\"Details\"].to_list()\n",
    "        elif i == 'blue':\n",
    "            df_blue =  index[index['Unid'].isin(clust_lst)]\n",
    "            blue_sp = df_blue[\"Details\"].to_list()\n",
    "        else:\n",
    "            break\n",
    "        print(len(clust_lst))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep, shows species that are in all three homologues\n",
    "\n",
    "#Makes a new dataframe from the three generated in the cell above\n",
    "rgb = red_sp + green_sp + blue_sp\n",
    "\n",
    "rgb_dict =  {i:rgb.count(i) for i in rgb}\n",
    "red_dict = {i:red_sp.count(i) for i in red_sp}\n",
    "green_dict = {i:green_sp.count(i) for i in green_sp}\n",
    "blue_dict = {i:blue_sp.count(i) for i in blue_sp}\n",
    "\n",
    "red_k = set(red_dict.keys())\n",
    "green_k = set(green_dict.keys())\n",
    "blue_k = set(blue_dict.keys())\n",
    "rgb_k = set(rgb_dict.keys())\n",
    "\n",
    "intersect_all = red_k & green_k & blue_k\n",
    "intersect_r_g = (red_k & green_k) - (red_k & green_k & blue_k)\n",
    "intersect_r_b = (red_k & blue_k) - (red_k & green_k & blue_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iall_df = index[index['Species'].isin(intersect_all)]\n",
    "ir_g_df = index[index['Species'].isin(intersect_r_g)]\n",
    "ir_b_df = index[index['Species'].isin(intersect_r_b)]\n",
    "\n",
    "iall_df['fasta'] = '>' + iall_df[\"Unid\"] + '|' + iall_df[\"Species\"] + \"|\" + iall_df[\"Full_Classification\"]+ '\\n' + iall_df[\"Sequence\"]\n",
    "ir_g_df['fasta'] = '>' + ir_g_df[\"Unid\"] + '|' + ir_g_df[\"Species\"] + \"|\" + ir_g_df[\"Full_Classification\"]+ '\\n' + ir_g_df[\"Sequence\"]\n",
    "ir_b_df['fasta'] = '>' + ir_b_df[\"Unid\"] + '|' + ir_b_df[\"Species\"] + \"|\" + ir_b_df[\"Full_Classification\"]+ '\\n' + ir_b_df[\"Sequence\"]\n",
    "\n",
    "iall = iall_df['fasta'].tolist()\n",
    "ir_g = ir_g_df['fasta'].tolist()\n",
    "ir_b = ir_b_df['fasta'].tolist()\n",
    "\n",
    "with open('Species_in_all.fasta', 'w') as ofile:\n",
    "    for i in iall:\n",
    "        ofile.write(i + '\\n')\n",
    "        \n",
    "with open('Species_in_SAL_AHL.fasta', 'w') as ofile:\n",
    "    for i in ir_g:\n",
    "        ofile.write(i + '\\n')\n",
    "\n",
    "with open('Species_in_SAL_Unid.fasta', 'w') as ofile:\n",
    "    for i in ir_b:\n",
    "        ofile.write(i + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a file that lists which species are found in all three clades, and which are found in SAL+AHL / SAL+Unid\n",
    "\n",
    "with open('Species_distro_across_homologues.txt', 'w') as ofile:\n",
    "    print(date_time + '\\n')\n",
    "    ofile.write(date_time + '\\n')\n",
    "    print('Species in all homologues: ')\n",
    "    ofile.write('Species in all homologues: \\n')\n",
    "    for k, v in rgb_dict.items():\n",
    "        if k in intersect_all:\n",
    "            print(str(v) + \"\\t\" + k)\n",
    "            ofile.write(str(v) + \"\\t\" + k + '\\n')\n",
    "    print(\"\")\n",
    "    ofile.write(\"\\n\")\n",
    "    print('Species in SAL & AHL (but not Unid): ')\n",
    "    ofile.write('Species in SAL & AHL (but not Unid): \\n')\n",
    "    for k, v in rgb_dict.items():\n",
    "        if k in intersect_r_g:\n",
    "            print(str(v) + \"\\t\" + k)\n",
    "            ofile.write(str(v) + \"\\t\" + k + '\\n')\n",
    "    print(\"\")\n",
    "    ofile.write(\"\\n\")\n",
    "    print('Species in SAL & Unid (but not AHL): ')\n",
    "    ofile.write('Species in SAL & AHL (but not Unid): \\n')\n",
    "    for k, v in rgb_dict.items():\n",
    "        if k in intersect_r_b:\n",
    "            print(str(v) + \"\\t\" + k)\n",
    "            ofile.write(str(v) + \"\\t\" + k + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cells generate a file with all the algal sequences in the phylogeny "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algae_lst = ['Rhodophyta', 'Chromista', 'Glaucophyta', 'Streptophyta', 'Chlorophyta']\n",
    "\n",
    "a_red = df_red.loc[df_red['Brief_Classification'].isin(algae_lst)]\n",
    "\n",
    "a_green = df_green.loc[df_green['Brief_Classification'].isin(algae_lst)]\n",
    "\n",
    "a_blue = df_blue.loc[df_blue['Brief_Classification'].isin(algae_lst)]\n",
    "\n",
    "\n",
    "\n",
    "algae_df = pd.concat([a_red, a_green, a_blue], join = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algae_df['Fasta'] = '>' + algae_df[\"Unid\"] + '|' + algae_df[\"Species\"] + \"|\" + algae_df[\"Brief_Classification\"]+ '\\n' + algae_df[\"Sequence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_lst = algae_df['Fasta'].tolist()\n",
    "\n",
    "with open('Algae_candidates.fasta', 'w') as ofile:\n",
    "    for i in a_lst:\n",
    "        ofile.write(str(i) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algae_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Ancestral Sequence Reconstruction (ASR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_clust2():\n",
    "    \"\"\"\n",
    "    Takes the Unids from the alignment used to make the phylogeny, and remakes the fasta file with \n",
    "    the raw sequences taken from the master_index\n",
    "    \"\"\"\n",
    "\n",
    "    cluster = ['red_f', 'green_f', 'blue_f']\n",
    "    index = pd.read_csv(\"wrk_index2.csv\", sep=\",\")\n",
    "    \n",
    "    for i in cluster:    \n",
    "        clust_lst = [] \n",
    "        with open(f'{i}.fasta', 'r') as clust:\n",
    "            for line in clust:\n",
    "                if line.startswith('>'):\n",
    "                    a = line.rstrip('\\n').lstrip('>')\n",
    "                    clust_lst.append(a)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "        # Makes a new index dataframe of unids found in subset\n",
    "        subset_df = index[index['Unid'].isin(clust_lst)]\n",
    "\n",
    "        subset_df['Unid'] = subset_df.Unid.astype(str)\n",
    "        subset_df['new_head'] = '>' + subset_df[\"Unid\"] + '_' + subset_df[\"Species\"] + \"_\" + subset_df[\"Brief_Classification\"]\n",
    "\n",
    "        # Converts subset_df into a dictionary where the key is the unid and the value is the sequence\n",
    "        fdict = dict(zip(subset_df.new_head, subset_df.Sequence))\n",
    "\n",
    "        ofile = open(f'ASR_{i}.fasta', 'w')            \n",
    "        # Writes a new fasta file with the unique ids found in the dictionary fdict\n",
    "        for x, y in fdict.items():\n",
    "            ofile.write('>' + str(x) + '\\n')\n",
    "            ofile.write(y + '\\n')\n",
    "\n",
    "        ofile.close()    \n",
    "        \n",
    "        log(f'ASR_{i}.fasta was created with {len(fdict)} sequences from the {i}.fasta file. This has the raw, unmodified sequences taken from wrk_index2.csv that are in the final phylogeny ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_clust2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['red', 'green', 'blue'] #refers to the final fasta files that made up the 3 clades in the 200909_profile_aln.fasta file that was used to build the\n",
    "                                 # GADI trees. Does not include the AtSALs\n",
    "\n",
    "#Makes a list of all unids in fasta file i\n",
    "\n",
    "for i in files:\n",
    "    ofile = open(f'00.ASR_aln_files/ASR_{i}.fasta', 'w')  \n",
    "    with open(f'00.ASR_aln_files/ASR_{i}_f_aln.fasta', 'r') as clust:\n",
    "        for line in clust:\n",
    "            if line.startswith('>'):\n",
    "                a = line.lstrip('>_')\n",
    "                ofile.write('>' + a[:5] + '\\n')\n",
    "            else:\n",
    "                ofile.write(line)\n",
    "\n",
    "    ofile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality control script to check that all sequences in the phylogeny are in the ASR alignment\n",
    "\n",
    "l1 = []\n",
    "l2 = []\n",
    "\n",
    "with open('tree_lst.txt', 'r') as tree_lst:\n",
    "    for i in tree_lst:\n",
    "        l1.append(str(i.rstrip('\\n')))\n",
    "\n",
    "with open('ASR_lst.fasta', 'r') as ASR:\n",
    "    for i in ASR:    \n",
    "        for line in ASR:\n",
    "            if line.startswith('>'):\n",
    "                a = line.rstrip().lstrip('>')\n",
    "                l2.append(a)\n",
    "                print(a)\n",
    "            else:\n",
    "                pass        \n",
    "            \n",
    "print(\"Sequences in tree but not aln: \")\n",
    "(set(l1) - set(l2))            \n",
    "            \n",
    "print(\"Sequences in aln but not tree: \")\n",
    "(set(l2) - set(l1))\n",
    "\n",
    "for i in l1:\n",
    "    if i == '18271':\n",
    "        print(i)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "print(len(l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Makes three fasta files (SAL, AHL, UIM) from the final aligned file, but from the 3 fasta files that have the same number of sequences for Machine Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes trimmed clusters (not aligned), generates a litst of unids and then takes the aligned files and makes a new fasta with the intersection\n",
    "# this new fasta file has the same actual sequences as the trimmed file but uses the aligned versions from the orignal file\n",
    "\n",
    "def final_clust(cluster):\n",
    "\n",
    "    clust_lst = []  \n",
    "\n",
    "    with open(f'{cluster}.fasta', 'r') as clust:\n",
    "        for line in clust:\n",
    "            if line.startswith('>'):\n",
    "                a = line.rstrip('\\n').lstrip('>')\n",
    "                a = str(a)\n",
    "                clust_lst.append(a)\n",
    "            else:\n",
    "                pass\n",
    "           \n",
    "    with open ('200909_profile_aln.fasta', 'r') as aln:\n",
    "\n",
    "        temp_dict = {}\n",
    "        x = ''\n",
    "        y = ''\n",
    "\n",
    "        # Converts the orignal, aligned and not trimmed, fasta file into a dictionary where the key is fasta header and the value is the sequence\n",
    "        for line in aln:\n",
    "\n",
    "            if line.startswith('>'):\n",
    "                line = line[:6]\n",
    "                x = line.strip('\\n').lstrip('>')\n",
    "                x = str(x)\n",
    "            else:\n",
    "                y = line.strip('\\n')\n",
    "            temp_dict.update({x: y}) \n",
    "\n",
    "        # Creates a new dictionary based off whether the unique id was found in the orignal SSN cluster csv\n",
    "        fdict = {x: y for x, y in temp_dict.items() if x in clust_lst}\n",
    "\n",
    "        ofile = open(f'{cluster}_f.fasta', 'w')        \n",
    "        # Writes a new fasta file with the unique ids found in the dictionary w_bdict\n",
    "        for x, y in fdict.items():\n",
    "            ofile.write('>' + str(x) + '\\n')\n",
    "            ofile.write(y + '\\n')\n",
    "\n",
    "        ofile.close()    \n",
    "\n",
    "    log(f'{cluster}_f.fasta was created with {len(fdict)} sequences from the 200909_profile_aln.fasta file - the final alignment. {cluster}.fasta is just a list of 150 sequences from that clade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clust('sal_ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes list of sequences from branches on the tree (from R) and then generates a fasta file of all sequences on that branch\n",
    "\n",
    "temp_dict = {}\n",
    "\n",
    "with open(f'ASR_lst.fasta', 'r') as fasta:\n",
    "        k = \"\"\n",
    "        v = \"\"\n",
    "        for line in fasta:\n",
    "            if line.startswith('>'):\n",
    "                k = line.rstrip().lstrip(\">\")\n",
    "            else:\n",
    "                v = line.strip('\\n')\n",
    "            temp_dict.update({k: v})    \n",
    "\n",
    "with open('AHL_split_clades.txt', 'r') as tips:\n",
    "    for i in tips:\n",
    "        dec = []\n",
    "        if i.startswith(\"Node:  \"):\n",
    "            anc = i[-5:]\n",
    "            anc = anc.strip()\n",
    "            anc = str(anc)\n",
    "        else:\n",
    "            dec.append(i.strip())\n",
    "        with open(f'Node_{anc}_decendents.fasta', 'a') as fasta:\n",
    "            w_dict = {k: v for k, v in temp_dict.items() if k in dec} \n",
    "            for k,v in w_dict.items():\n",
    "                fasta.write('>' + k + '\\n')\n",
    "                fasta.write(v + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes a list of unids and then makes a fasta file from the original unedited sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orig(node):\n",
    "\n",
    "    index = pd.read_csv(\"Phytozome_only.csv\", sep=\",\")\n",
    "\n",
    "    dec = []\n",
    "\n",
    "    with open(f'node_{node}.txt', 'r') as tips:\n",
    "        for i in tips:\n",
    "            dec.append(int(i.strip()))\n",
    "    print(dec)\n",
    "    # Converts subset_df into a dictionary where the key is the unid and the value is the sequence\n",
    "    fdict = dict(zip(index.Unid, index.Sequence))       \n",
    "    with open(f'Node_{node}_decendents.fasta', 'a') as fasta:\n",
    "        w_dict = {k: v for k, v in fdict.items() if k in dec} \n",
    "        for k,v in w_dict.items():\n",
    "            fasta.write('>' + str(k) + '\\n')\n",
    "            fasta.write(v + '\\n')       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90464, 81871, 33984, 25695, 41913, 87157, 23667, 66549, 52038, 62990, 97858, 47245, 63293, 58119, 38520, 80724, 94141, 98974, 77267, 96564, 92671, 54419, 56488, 54601, 73058, 57806, 80579, 97730, 17105, 76271, 20909, 93553, 23732, 51398, 99371, 90543, 17083, 81896, 31117, 69989, 79765, 21743, 89136, 66767, 61883, 33970, 84340, 49821, 95740, 66145, 59090, 19472, 26614, 36524, 29668, 82788, 29175, 59641, 20126, 23433, 89288, 56570, 84126, 65326, 31950, 69093, 76470, 49138, 81778, 19111, 53402, 61760, 83804, 85491, 90152, 82085, 14745, 86606, 77008, 71776, 73908, 43082, 73472, 19707, 54413, 77394, 61911, 12096, 48080, 52894, 82438, 31442, 67835, 56100, 64520, 44350, 18248, 90688, 50775, 67960, 57679, 60898, 30415, 79889, 51878, 60947, 60798, 68115, 41591, 90087, 95143, 60720, 50477, 73944, 97561, 95815, 21163, 33450, 17937, 74279, 24967, 51263, 62592, 76260, 58274, 31151, 31679, 16761, 98862, 81650, 49793, 94525, 40121, 54197, 47032, 57719, 33655, 30263, 54625, 85154, 76490, 76307, 28130, 31520, 38327, 91219, 87388, 66021, 31433, 37055, 61083, 35362, 46716, 33555, 90131]\n"
     ]
    }
   ],
   "source": [
    "orig(519)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Protein analysis. Takes a list of fasta files of amino acid sequences and generates a csv with sequence attributes:\n",
    "* Unid\n",
    "* Sequence length\n",
    "* Aromaticity \n",
    "* Instability point (iI)\n",
    "* Isoelectric point (pI)\n",
    "* Extinction coefficient (eC)\n",
    "* Clade (the fasta file name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes list of fasta files uses BioPython to calculate attributes\n",
    "def seq_analysis(*args):\n",
    "    \n",
    "    # Takes the list of fasta files as ints from files named \"Node_{int}_decendents.fasta\"\n",
    "    clade_lst = args   \n",
    "    \n",
    "    #Just makes a string of the clade number for labelling the final file\n",
    "    clades = ''\n",
    "    for clade in clade_lst:\n",
    "        clades = clades + '_' + str(clade)\n",
    "    \n",
    "    # Initialises a list for appending dictionaries of \n",
    "    protein_features = []\n",
    "   \n",
    "    # Iterates through list of files/clades\n",
    "    for i in clade_lst:\n",
    "        with open(f\"Node_{i}_decendents.fasta\") as handle:\n",
    "            # Parses fasta file with BioPython\n",
    "            for record in SeqIO.parse(handle, \"fasta\"):\n",
    "                temp_dict = {}\n",
    "\n",
    "                # Calculates properties of each sequence\n",
    "                k = record.id\n",
    "                length = len(record.seq)\n",
    "                analysed_seq = ProteinAnalysis(str(record.seq))\n",
    "                aro = analysed_seq.aromaticity()\n",
    "                iI = analysed_seq.instability_index()\n",
    "                pI = analysed_seq.isoelectric_point()\n",
    "                eC = analysed_seq.molar_extinction_coefficient()[0]\n",
    "\n",
    "                # Uses Pandas to create a dictionary with property names as the keys and values as the value\n",
    "                # for each property it appends to the dictionary 'temp_dict'\n",
    "                temp_dict['Unid'] = k\n",
    "                temp_dict['Length'] = length\n",
    "                temp_dict['Aromaticity'] = aro\n",
    "                temp_dict['Instability_point'] = iI\n",
    "                temp_dict['Isoelectric_point'] = pI\n",
    "                temp_dict['Extinction_coefficient'] = eC\n",
    "                temp_dict['Clade'] = i\n",
    "                \n",
    "                # Adds dictionary to the list 'protein_features'\n",
    "                protein_features.append(temp_dict)\n",
    "\n",
    "    # Pandas takes the list 'protein_features' and coverts each dictionary into a row in the dataframe 'df'\n",
    "    # then prints the df to csv\n",
    "    df = pd.DataFrame.from_dict(protein_features)\n",
    "    df.to_csv(f'clades{clades}_stats.csv', sep=',', index=False)\n",
    "    \n",
    "#run t-tests in R comparing the clades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_analysis(693,734)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the localisation data for a specific set of tips from a branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes a csv file 'master_loc' manulally created fro DeepLoc1.0 output with columns called 'Unid' and 'Localisation'\n",
    "# It then writes a new localisation info csv file with a subset of Unid's taken from a text file with only Unid's (one per row)\n",
    "\n",
    "def loc(node):\n",
    "\n",
    "    index = pd.read_csv(\"master_loc.csv\", sep=\",\")\n",
    "\n",
    "    dec = []\n",
    "\n",
    "    # Generates a list 'dec' of Unids (tips) from the node file\n",
    "    with open(f'node_{node}.txt', 'r') as tips:\n",
    "        for i in tips:\n",
    "            dec.append(i.strip())\n",
    "\n",
    "    # Converts 'index' into a dictionary where the key is the unid and the value is the localisation\n",
    "    fdict = dict(zip(index.Unid.astype(str), index.Localisation))   \n",
    "    # Creates/appends a csv file\n",
    "    with open(f'Node_{node}_localisation.csv', 'a') as doc:\n",
    "        # Creates a dictionary (w_dict) but only if the key in fdict matches an element in list 'dec'\n",
    "        w_dict = {k: v for k, v in fdict.items() if k in dec} \n",
    "        # Unpacks w_dict and writes to the csv with the key (Unid) seperated with the value (Localisation) seperated by a coma per row\n",
    "        for k,v in w_dict.items():\n",
    "            doc.write(k + ',' + v + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc(693)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
