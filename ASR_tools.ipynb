{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ancestral Sequence Reconstruction \n",
    "Original code from Matt Spence, Mahakaran Sandhu, Josh Mitchell (Jackson Lab), modified by Matt Mortimer, matthew.mortimer@anu.edu.au, ORCID ID: https://orcid.org/0000-0002-8135-9319"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "from Bio import AlignIO\n",
    "from Bio import SeqIO\n",
    "from log_func import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables that change with each project\n",
    "\n",
    "project = 'SAL-AHL'\n",
    "len_seq = 751 #Length of the codeml alignment (equal to the input alignment)\n",
    "node_lst = []\n",
    "\n",
    "in_rst = 'rst'\n",
    "sub_dir = 'ASR'\n",
    "f_path = 'ASR/rst'\n",
    "codeml_file = 'Anc_seqs_codeml.fasta'\n",
    "grasp_file = 'SAL-AHL_grasp_201028_joint-ancestors_GRASP.fasta'\n",
    "ancNode_lst = [348,521,519,515,514,509,507,368,362,361,467,349]  # List of ancestral nodes for reconstruction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_anc_trees(infile):\n",
    "    \"\"\"Retreives trees from CodeML RST file. Two trees will be returned, the first will include\n",
    "    the ancestral node labels but no branchlengths, the second will include branch lengths but\n",
    "    no node labels. Note that bootstraps are not stored in either tree and will have to be \n",
    "    retrieved from the in-tree file. Both trees have identical topologies, so branch lengths/\n",
    "    node labels can be exchanged between them using tree manipulation packages. Credit to Matt S\"\"\"\n",
    "    trees = []\n",
    "    with open(infile) as codeML:\n",
    "        for line in codeML:\n",
    "            if line.startswith('(') and line.endswith(';\\n'):\n",
    "                trees.append(line.strip())    \n",
    "    anc_nwk_bl = trees[0]\n",
    "    anc_nwk_lab = trees[2]\n",
    "    return(anc_nwk_lab, anc_nwk_bl)\n",
    "\n",
    "def read_rst(infile, node_point, seq_length):\n",
    "    \"\"\"Reads to CodeML RST file and returns the posterior probability of a SINGLE node (designated by node_point).\n",
    "    Note that read_rst returns a tuple with (node_point, node data), node data includes ALL raw sequence info\n",
    "    from the RST file, not the extracted extracted float values corresponding to the posterior distribution.\n",
    "    \n",
    "    read_rst is intended to be run on a SINGLE node at a time and will not take a list of nodes to read. Instead,\n",
    "    construct a for-loop of nodes and call read_rst once for each node. Credit to Matt S and Mahakaran\"\"\"\n",
    "    \n",
    "    data = open(infile, 'r')\n",
    "    datas = data.readlines()\n",
    "    cat_data = []\n",
    "    for i in range(len(datas)):\n",
    "        node_data = []\n",
    "        if 'Prob distribution at node' in datas[i]:\n",
    "            if node_point > 999:                      # Why is this '999'?\n",
    "                if ',' in datas[i][26:30]:\n",
    "                    continue\n",
    "                else:\n",
    "                    if int(datas[i][26:30]) == node_point:\n",
    "                        node_data.append(datas[i+4:i+4+int(seq_length)])\n",
    "                        node = datas[i][26:30]\n",
    "                        node_data = node_data[0]\n",
    "                        nodes = (node, node_data)\n",
    "                        cat_data.append(nodes)\n",
    "\n",
    "            else:\n",
    "                if int(datas[i][26:29]) == node_point:\n",
    "                    node_data.append(datas[i+4:i+4+int(seq_length)])\n",
    "                    node = datas[i][26:29]\n",
    "                    node_data = node_data[0]\n",
    "                    nodes = (node, node_data)\n",
    "                    cat_data.append(nodes)\n",
    "    return cat_data\n",
    "\n",
    "\n",
    "def pd_post_dist(rst):    \n",
    "    \"\"\"TAKES THE OUTPUT OF read_rst command. Retrieves the posterior distribution of a node, and returns\n",
    "    it as a pandas dataframe of floating values indexed by site of the sequence. Note that pd_post_dist will\n",
    "    not return an ML sequence, only the numerical values of the full posterior distribution.\n",
    "    \n",
    "    pd_post_dist is intended to be run on a SINGLE output from read_rst and will not parse a list of rst values.\n",
    "    Instead, construct a for loop with read_rst and pd_post_dist. Credit to Matt S\"\"\"\n",
    "    for i in rst:\n",
    "        data = i[1]\n",
    "    aa_site_list = []\n",
    "    aa_post_dict = []\n",
    "    for site, i in enumerate(data):\n",
    "        z = []\n",
    "        for m, n in enumerate(i.split(':')):\n",
    "            if m == 1:\n",
    "                z.append(n.strip())\n",
    "                d = {site+1:z}\n",
    "                aa_post_dict.append(d)\n",
    "    for ind, dist in enumerate(aa_post_dict):\n",
    "        if ind == 0:\n",
    "            df = pd.DataFrame.from_dict(dist).transpose()\n",
    "        else:\n",
    "            df = pd.concat([df, pd.DataFrame.from_dict(dist).transpose()], axis = 0, sort = False) \n",
    "    df = df[0].str.split(' ', n = 19, expand = True)\n",
    "    for col in range(20):    \n",
    "        test_list = df[col].to_list()\n",
    "        z = []\n",
    "        for i in test_list:\n",
    "            z.append(re.search('\\(([^)]+)', i).group(1))\n",
    "        df[col] = z\n",
    "    return df\n",
    "\n",
    "def pd_ML_residues(infile, nodes, seq_length): \n",
    "    \"\"\"DOES NOT TAKE THE OUTPUT OF read_rst or pd_post_dist. pd_ML_residues calls read_rst, pd_post_dist commands to \n",
    "    retrieve the ML probabilities for each sequence in a list of nodes and returns them as rows of a pd dataframe\n",
    "    indexed by node. Note that this will not return the ML sequence, only the ML probabilities at each site of each\n",
    "    node.\n",
    "    \n",
    "    pd_ML_residues is intended to be run on a LIST of MULTIPLE node numbers, to allow easy manipulation of sites \n",
    "    that contribute to the single mean posterior probability. Credit to Matt S\"\"\"\n",
    "    for i, node in enumerate(nodes):\n",
    "        rst = read_rst(infile, node_point=int(node), seq_length=len_seq)\n",
    "        df = pd_post_dist(rst)\n",
    "        df['ML'] = df.max(axis =1)\n",
    "        ML_prob = []\n",
    "        ML_prob = df['ML'].to_list()\n",
    "        if i == 0:\n",
    "            ml_df = pd.DataFrame(ML_prob).transpose()\n",
    "        else:\n",
    "            ml_df = pd.concat([ml_df, pd.DataFrame(ML_prob).transpose()], axis = 0)\n",
    "    ml_df.columns = list(range(1, seq_length+1))\n",
    "    ml_df.index = nodes\n",
    "    return ml_df\n",
    "\n",
    "\n",
    "def branch_mutations(infile, no_branches):\n",
    "    \"\"\"Reads the codeML RST file and returns a dictionary of mutations (including posteriors)\n",
    "    per branch and a pandas dataframe with branch and '/' separated list of mutations (not\n",
    "    including posteriors). No_branches is the number of branches, which is equal to the \n",
    "    number of tips - 2. \n",
    "    \n",
    "    branch_mutations() will retrieve mutations from all branches in the tree. For individual\n",
    "    branches, use the output dictionary with the branch number. Branches with no mutations\n",
    "    are replaced by np.nan. Credit to Matt S and Josh.\"\"\"\n",
    "    with open(infile, 'r') as file:\n",
    "        branches_out = {}\n",
    "        current_branch = None\n",
    "        for line in file.readlines():\n",
    "            if line.startswith(\"Branch \"):\n",
    "                current_branch = int(line.split()[1][:-1])\n",
    "                branch_list = branches_out.setdefault(current_branch, [])\n",
    "            elif line.strip() == \"List of extant and reconstructed sequences\":\n",
    "                break\n",
    "            elif line.strip() and current_branch:\n",
    "                branch_list.append(line.split())\n",
    "    new_dict = {}\n",
    "    for key, value in branches_out.items():\n",
    "        new_dict[key] = pd.DataFrame(value) \n",
    "    branch = []\n",
    "    muts_list = []       \n",
    "    for x in range(1, (no_branches+1)):\n",
    "        z = []\n",
    "        df = new_dict[x]\n",
    "        if df.empty == True:\n",
    "            continue\n",
    "        else:\n",
    "            df = new_dict[x]\n",
    "            df[4] = df[4].replace({'-':np.nan})\n",
    "            df = df.dropna()\n",
    "            pos = df[0].tolist()\n",
    "            ori = df[1].tolist()\n",
    "            mut = df[4].tolist()\n",
    "            for p,o,m in zip(pos, ori, mut):\n",
    "                comb = str(o)+str(p)+str(m)\n",
    "                z.append(comb) \n",
    "                y = '/'.join(z)\n",
    "            branch.append(x)\n",
    "            muts_list.append(y)\n",
    "    branch_muts_df = pd.DataFrame(branch)\n",
    "    branch_muts_df[1] = muts_list\n",
    "    branch_muts_df.columns = ['branch', 'mutations']\n",
    "    return new_dict, branch_muts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asr_processing(len_seq = len_seq, project = project, sub_dir = ''):\n",
    "    '''\n",
    "    Takes the following args\n",
    "    'project' = project name as str\n",
    "    'sub_dir' = subdir name of where rst file is location as str\n",
    "    'len_seq' = the length of the alignment used for the ASR as int\n",
    "    1) Retreives the two ancestral trees from the codeml 'rst' file and writes them to file as newick\n",
    "    2) Takes the list of ancestral nodes of interest generates heatmaps of site by Amino acid, then \n",
    "        saves them to file\n",
    "    Outputs:\n",
    "        {project}_anc_lab.nwk\n",
    "        {project}_anc_bl.nwk\n",
    "        {anc}_HTH_PP.pdf where anc is each ancestral node in ancNode_lst\n",
    "        Anc_seqs_codeml.fasta with all the ancestral sequences from the rst file\n",
    "        ML_seq_df a dataframe of the ML probability/site for each node in the tree\n",
    "    '''\n",
    "\n",
    "    #Retreives the two anc trees from the RST and writing them to file\n",
    "\n",
    "#     anc_lab, anc_bl = get_anc_trees(f'{f_path}')\n",
    "#     print(anc_lab, file = open(f'{f_path}_anc_lab.nwk', 'a'))\n",
    "#     print(anc_bl, file = open(f'{f_path}_anc_bl.nwk', 'a'))\n",
    "    \n",
    "    \n",
    "#     for n in ancNode_lst:\n",
    "#         anc = str(n)\n",
    "#         anc_temp = read_rst(f'{f_path}',n, len_seq) #seq_length (3rd variable) is this just the alignment length?\n",
    "#         anc_temp_pd = pd_post_dist(anc_temp)\n",
    "#         anc_temp_pd.head(100)\n",
    "#         anc_temp_pd = anc_temp_pd.astype(float)\n",
    "#         aa = ['A','R','N','D','C','Q','E','G','H','I','L','K','M','F','P','S','T','W','Y','V']\n",
    "#         plt.pcolor(anc_temp_pd, cmap='viridis')\n",
    "#         plt.xlabel('Amino acid', fontsize = 10) \n",
    "#         plt.xticks(np.arange(0.5, 20.5, step=1), aa)\n",
    "#         plt.ylabel('Site', fontsize = 10)\n",
    "#         plt.title(f'{anc} posterior distribution', fontsize = 14)\n",
    "#         plt.savefig(f'Anc{anc}_PP.pdf')\n",
    "#         log(f'Anc{anc}_PP.pdf was created for {project} ASR')\n",
    "    \n",
    "#     # Converting codeml seqs to fasta \n",
    "#     ofile = open(f'ASR/Anc_seqs_codeml.fasta', 'a')\n",
    "\n",
    "#     with open(f'{f_path}') as rst:\n",
    "#         count = 0\n",
    "#         for i in rst:\n",
    "#             if i.startswith('node #'):\n",
    "#                 n_id = i[6:10]\n",
    "#                 n_id = n_id.strip()\n",
    "#                 node_lst.append(int(n_id))\n",
    "#                 seq = i[10:]\n",
    "#                 seq = ''.join(seq.split())\n",
    "#                 ofile.write('>Anc_' + n_id + '\\n')\n",
    "#                 ofile.write(seq + '\\n')\n",
    "#                 count += 1\n",
    "#     log(f'Anc_seqs_codeml.fasta was created with {count} ancestral sequences')\n",
    "#     ofile.close()\n",
    "    \n",
    "#     aln_codeml = AlignIO.read(f'ASR/{codeml_file}', 'fasta')\n",
    "#     aln_grasp = AlignIO.read(f'ASR/{grasp_file}', 'fasta')\n",
    "\n",
    "#     seq_ids = []\n",
    "#     seqs = []\n",
    "#     for i in range(len(aln_grasp)):\n",
    "#         test_grasp = aln_grasp[i]\n",
    "#         test_codeml = aln_codeml[i]\n",
    "#         seq_ids.append(aln_codeml[i].id)\n",
    "#         partial_seq = ''\n",
    "#         for n,(g,c) in enumerate(zip(test_grasp,test_codeml)):\n",
    "#             if g == '-':\n",
    "#                 partial_seq+='-'\n",
    "#             else:\n",
    "#                 partial_seq+=c\n",
    "#         seqs.append(partial_seq)\n",
    "    \n",
    "#     c_count = 0\n",
    "#     for i,s in zip(seq_ids,seqs):\n",
    "#         print('>'+i, file=open(f'ASR/Anc_seqs_corrected.txt', 'a'))\n",
    "#         print(s, file=open(f'ASR/Anc_seqs_corrected.txt', 'a'))\n",
    "#         c_count += 1\n",
    "#     log(f'Anc_seqs_corrected.txt was created with {c_count} ancestral sequences')\n",
    "\n",
    "    pp_dists = []\n",
    "    aln_codeml = AlignIO.read(f'ASR/Anc_seqs_corrected.txt', 'fasta')\n",
    "    anc_nodes = node_lst \n",
    "    for i in anc_nodes:\n",
    "        pp_rst = read_rst(infile= f_path, node_point=int(i), seq_length=len_seq)\n",
    "        pp_dist = pd_post_dist(pp_rst)\n",
    "        pp_dists.append(pp_dist)\n",
    "\n",
    "    all_sites = []\n",
    "    for seq in aln_codeml:\n",
    "        sites_to_drop = []\n",
    "        for n,aa in enumerate(seq.seq):\n",
    "            if aa == '-':\n",
    "                sites_to_drop.append(n+1)\n",
    "            else:\n",
    "                continue\n",
    "        all_sites.append(sites_to_drop)\n",
    "\n",
    "    corrected_pp_dists = []\n",
    "    ML_pp = []\n",
    "    for i,n in zip(all_sites,pp_dists):\n",
    "            pp_dist = n.drop(i, axis = 0)\n",
    "            pp_dist['ML'] = pp_dist.max(axis =1)\n",
    "            ML_prob = pp_dist['ML'].to_list()\n",
    "            ML_pp.append(ML_prob)\n",
    "            corrected_pp_dists.append(pp_dist)\n",
    "    \n",
    "    global mean_pp\n",
    "    mean_pp = []\n",
    "    for i in ML_pp:\n",
    "        av = sum(i)/len(i)\n",
    "        mean_pp.append(av)\n",
    "\n",
    "#     for i,n in zip(aln_codeml,mean_pp):\n",
    "#         print(i.id+'\\t'+str(n), file=open(f'ASR/Anc_mean_pp.txt', 'a')) \n",
    "#     log(f'Anc_mean_pp.txt was created with the mean posterior probablities for each seqeunce in Anc_seqs_corrected.txt')\n",
    "    \n",
    "    \n",
    "#     anc_tree,ex_tree = get_anc_trees(f_path)\n",
    "#     print(anc_tree, file = open(f'ASR/Anc_tree.txt.treefile', 'a'))\n",
    "#     log(f'Anc_tree.txt.treefile phylogentic tree was created in newick format')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runs the ASR processing\n",
    "Merges codeml and grasp alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two charts with posterior probabilities of the codeml and then the corrected ancestors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{f_path}') as rst:\n",
    "    ML_seq_df = pd_ML_residues(infile=f_path, nodes=node_lst, seq_length=len_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col = list(range(1,751))\n",
    "new_col.append('MPP')\n",
    "ML_seq_df.columns = new_col\n",
    "\n",
    "\n",
    "cm = plt.cm.get_cmap('viridis')\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "# Plot histogram.\n",
    "n, bins, patches = plt.hist(ML_seq_df['MPP'], bins=20, edgecolor = 'black', linewidth = 1, zorder = 100)\n",
    "bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "\n",
    "# scale values to interval [0,1]\n",
    "col = bin_centers - min(bin_centers)\n",
    "col /= max(col)\n",
    "\n",
    "for c, p in zip(col, patches):\n",
    "    plt.setp(p, 'facecolor', cm(c))\n",
    "    \n",
    "plt.xlabel(\"Mean posterior probability\", fontsize=8)  \n",
    "plt.ylabel(\"Number of nodes\", fontsize=8)\n",
    "plt.xticks(fontsize=8)  \n",
    "plt.yticks(fontsize=8)\n",
    "plt.tick_params(axis='x', labelsize=8)\n",
    "plt.tick_params(axis='y', labelsize=8)\n",
    "\n",
    "ax.axvline(0.9, linewidth =1,alpha=1, color='gray', linestyle = 'dashed')\n",
    "\n",
    "fig.set_size_inches(3.05,1.95)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('codeml_mean_pp.pdf', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = plt.cm.get_cmap('viridis')\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "# Plot histogram.\n",
    "n, bins, patches = plt.hist(mean_pp, bins=20, edgecolor = 'black', linewidth = 1, zorder = 100)\n",
    "bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "\n",
    "# scale values to interval [0,1]\n",
    "col = bin_centers - min(bin_centers)\n",
    "col /= max(col)\n",
    "\n",
    "for c, p in zip(col, patches):\n",
    "    plt.setp(p, 'facecolor', cm(c))\n",
    "\n",
    "plt.xlabel(\"Mean posterior probability\", fontsize=8)  \n",
    "plt.ylabel(\"Number of nodes\", fontsize=8)\n",
    "plt.xticks(fontsize=8)  \n",
    "plt.yticks(fontsize=8)\n",
    "plt.tick_params(axis='x', labelsize=8)\n",
    "plt.tick_params(axis='y', labelsize=8)\n",
    "\n",
    "ax.axvline(0.9, linewidth =1,alpha=1, color='gray', linestyle = 'dashed')\n",
    "\n",
    "fig.set_size_inches(3.05,1.95)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('corrected_mean_pp.pdf', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make fasta files of each ancestral node from the txt file 'Anc_tips.txt' from R (packages caper and ape), uses biopython to generate consensus sequences and then prints to file (Anc_consensus_equiv.fasta)\n",
    "\n",
    "I need to add Anc_consensus_equiv.fasta to the ancestors and extant sequences file then generate the corelation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit Matt Mortimer, work in progress - make into a \n",
    "\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "\n",
    "files = os.listdir()\n",
    "\n",
    "master_dict = {}\n",
    "\n",
    "# 'ASR_lst.fasta' is the alignment that the ancestral sequences were made from \n",
    "# Generates a dictionary of Unids and Sequences called 'master_dict'\n",
    "with open(f'ASR_lst.fasta', 'r') as fasta:\n",
    "        k = \"\"\n",
    "        v = \"\"\n",
    "        for line in fasta:\n",
    "            if line.startswith('>'):\n",
    "                k = line.rstrip().lstrip(\">\")\n",
    "            else:\n",
    "                v = line.strip('\\n')\n",
    "            master_dict.update({k: v})    \n",
    "\n",
    "# 'Anc_tips.txt' is a file of all decendants/tips from a given ancestor/node generated in R\n",
    "# Takes the ancestor number from the file and converts to a str to use in the file name\n",
    "with open('Anc_tips.txt', 'r') as tips:\n",
    "    for i in tips:\n",
    "        dec = []\n",
    "        if i.startswith(\"Ancestor:  \"):\n",
    "            anc = i[-5:]\n",
    "            anc = anc.strip()\n",
    "            anc = str(anc)\n",
    "        else:\n",
    "            # All tip numbers / unids are added as elements to list 'dec'\n",
    "            dec.append(i.strip())\n",
    "        \n",
    "        # Creates/appends to a fasta file sequences taken from 'master_dict' if in the list 'dec'\n",
    "        with open(f'Anc_{anc}_decendents.fasta', 'a') as fasta:\n",
    "            w_dict = {k: v for k, v in master_dictt.items() if k in dec} \n",
    "            for k,v in w_dict.items():\n",
    "                fasta.write('>' + k + '\\n')\n",
    "                fasta.write(v + '\\n')\n",
    "\n",
    "# Uses BioPython to make concensus sequences from the 'Anc_{anc}_decendents.fasta' generated above          \n",
    "for f in files:\n",
    "    if f.startswith(\"Anc_tip\"):\n",
    "        pass\n",
    "    # If the file is not an 'Anc_tip' file it will proceed to below. Can't have anything else in the dir other\n",
    "    # than 'Anc_tip' or 'Anc_{anc}_decendents.fasta' files or it will fail\n",
    "    elif f.startswith(\"Anc_\"):\n",
    "        # Take the name of the Ancestor the sequences are decendants of and save as variable 'anc' \n",
    "        anc = f[:7]\n",
    "        # Uses BioPython / AlignIO to parse the fasta file then 'summerises' it assigning the object as variable 'summary_align'\n",
    "        aln = AlignIO.read(f, 'fasta')\n",
    "        summary_align = AlignInfo.SummaryInfo(aln)\n",
    "        # Generates a concensus sequence, assigns it to variable 'consensus'\n",
    "        consensus = summary_align.gap_consensus(threshold=0, ambiguous='X', consensus_alpha=None, require_multiple=0)\n",
    "        print(str(consensus))\n",
    "        # Creates or appends file 'Anc_consensus_equiv.fasta'. Each concesus sequence has a fasta header labelled by the ancestor equiv\n",
    "        with open(f'Anc_consensus_equiv.fasta', 'a') as con:\n",
    "            con.write(f\">{anc}_consensus\\n\")\n",
    "            con.write(str(consensus) + \"\\n\")\n",
    "    else:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
